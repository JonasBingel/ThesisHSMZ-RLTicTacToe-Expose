
@thesis{allisSearchingSolutionsGames1994,
  title = {Searching for solutions in games and artificial intelligence},
  author = {Allis, Louis Victor},
  date = {1994},
  institution = {{Ponsen \& Looijen}},
  location = {{Wageningen}},
  isbn = {9789090074887},
  langid = {Summary in Dutch},
  annotation = {OCLC: 60586781},
  file = {C\:\\Users\\Bingel\\Zotero\\storage\\YN2U4Q8X\\Allis_1994_Searching for solutions in games and artificial intelligence.pdf}
}

@book{bellmanDynamicProgramming1957,
  title = {Dynamic Programming},
  author = {Bellman, Richard},
  date = {1957},
  publisher = {{Princeton Univ. Pr}},
  location = {{Princeton, NJ}},
  isbn = {978-0-691-07951-6},
  pagetotal = {339},
  file = {C\:\\Users\\Bingel\\Zotero\\storage\\YP5J39VP\\Bellman_1984_Dynamic programming.pdf}
}

@article{bellmanTheoryDynamicProgramming1954,
  title = {The Theory of Dynamic Programming},
  author = {Bellman, Richard},
  date = {1954},
  journaltitle = {Bulletin of the American Mathematical Society},
  shortjournal = {Bull. Amer. Math. Soc.},
  volume = {60},
  number = {6},
  pages = {503--515},
  doi = {10.1090/S0002-9904-1954-09848-8},
  langid = {english},
  file = {C\:\\Users\\Bingel\\Zotero\\storage\\LE778HC6\\Bellman_1954_The theory of dynamic programming.pdf}
}

@report{bowlingConvergenceNoRegretMultiagent2004,
  title = {Convergence and {{No-Regret}} in {{Multiagent Learning}}},
  author = {Bowling, Michael},
  date = {2004},
  institution = {{University of Alberta Libraries}},
  doi = {10.7939/R3ZS2KF41},
  file = {C\:\\Users\\Bingel\\Zotero\\storage\\Q5UHAWCR\\Bowling_2004_Convergence and No-Regret in Multiagent Learning.pdf}
}

@report{boyanModularNeuralNetworks1992,
  title = {Modular Neural Networks for Learning Context-Dependent Game Strategies},
  author = {Boyan, Justin A.},
  date = {1992},
  institution = {{Master’s thesis, Computer Speech and Language Processing}},
  abstract = {by},
  file = {C\:\\Users\\Bingel\\Zotero\\storage\\G2AFAH2H\\Boyan_1992_Modular neural networks for learning context-dependent game strategies.pdf;C\:\\Users\\Bingel\\Zotero\\storage\\NJJZ6ELM\\summary.html}
}

@article{demingTheoryGamesEconomic1945a,
  title = {Theory of {{Games}} and {{Economic Behavior}}.},
  author = {Deming, W. Edwards and von Neumann, John and Morgenstern, Oscar},
  options = {useprefix=true},
  date = {1945-06},
  journaltitle = {Journal of the American Statistical Association},
  shortjournal = {Journal of the American Statistical Association},
  volume = {40},
  number = {230},
  eprint = {2280142},
  eprinttype = {jstor},
  pages = {263},
  issn = {01621459},
  doi = {10.2307/2280142}
}

@article{epsteinIdealTrainer1994,
  title = {Toward an Ideal Trainer},
  author = {Epstein, Susan L.},
  date = {1994-06},
  journaltitle = {Machine Learning},
  shortjournal = {Mach Learn},
  volume = {15},
  number = {3},
  pages = {251--277},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/BF00993346},
  langid = {english},
  file = {C\:\\Users\\Bingel\\Zotero\\storage\\NEQ3X72F\\Epstein_1994_Toward an ideal trainer.pdf}
}

@book{ertelIntroductionArtificialIntelligence2017,
  title = {Introduction to {{Artificial Intelligence}}},
  author = {Ertel, Wolfgang},
  date = {2017},
  series = {Undergraduate {{Topics}} in {{Computer Science}}},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-58487-4},
  langid = {english},
  file = {C\:\\Users\\Bingel\\Zotero\\storage\\ZGWAS38E\\Ertel_2017_Introduction to Artificial Intelligence.pdf}
}

@book{howardDynamicProgrammingMarkov1960,
  title = {Dynamic Programming and {{Markov}} Processes},
  author = {Howard, Ronald A.},
  date = {1960},
  edition = {6. print},
  publisher = {{M.I.T. Pr}},
  location = {{Cambridge, Mass}},
  isbn = {978-0-262-08009-5},
  langid = {english},
  pagetotal = {136},
  file = {C\:\\Users\\Bingel\\Zotero\\storage\\QK7NG7XJ\\Howard_1960_Dynamic programming and Markov processes.pdf}
}

@inproceedings{konenParameterSelection2008,
  title = {Reinforcement Learning: {{Insights}} from Interesting Failures in Parameter Selection},
  booktitle = {Parallel Problem Solving from Nature – {{PPSN}} x},
  author = {Konen, Wolfgang and Bartz–Beielstein, Thomas},
  date = {2008},
  pages = {478--487},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  isbn = {978-3-540-87700-4},
  file = {C\:\\Users\\Bingel\\Zotero\\storage\\LB959ZBM\\Konen_Bartz–Beielstein_2008_Reinforcement learning.pdf}
}

@inproceedings{littmanMarkovGamesFramework1994,
  title = {Markov Games as a Framework for Multi-Agent Reinforcement Learning},
  booktitle = {In {{Proceedings}} of the {{Eleventh International Conference}} on {{Machine Learning}}},
  author = {Littman, Michael L.},
  date = {1994},
  pages = {157--163},
  doi = {10.1.1.48.8623},
  publisher = {{Morgan Kaufmann}},
  file = {C\:\\Users\\Bingel\\Zotero\\storage\\MALGHUX9\\Littman_1994_Markov games as a framework for multi-agent reinforcement learning.pdf;C\:\\Users\\Bingel\\Zotero\\storage\\UPQ6WYZV\\summary.html}
}

@article{netoSingleAgentMultiAgentReinforcement,
  title = {From {{Single-Agent}} to {{Multi-Agent Reinforcement Learning}}: {{Foundational Concepts}} and {{Methods}}},
  author = {Neto, Goncalo},
  pages = {},
  langid = {english},
  file = {C\:\\Users\\Bingel\\Zotero\\storage\\VKHR52UV\\Neto - From Single-Agent to Multi-Agent Reinforcement Lea.pdf}
}

@article{rummeryOnLineQLearningUsing1994,
  title = {On-{{Line Q-Learning Using Connectionist Systems}}},
  author = {Rummery, G. and Niranjan, Mahesan},
  date = {1994-11-04},
  journaltitle = {Technical Report CUED/F-INFENG/TR 166},
  shortjournal = {Technical Report CUED/F-INFENG/TR 166},
  abstract = {Reinforcement learning algorithms are a powerful machine learning technique. However, much of the work on these algorithms has been developed with regard to discrete finite-state Markovian problems, which is too restrictive for many real-world environments. Therefore, it is desirable to extend these methods to high dimensional continuous state-spaces, which requires the use of function approximation to generalise the information learnt by the system. In this report, the use of back-propagation neural networks (Rumelhart, Hinton and Williams 1986) is considered in this context. We consider a number of different algorithms based around Q-Learning (Watkins 1989) combined with the Temporal Difference algorithm (Sutton 1988), including a new algorithm (Modified Connectionist Q-Learning), and Q() (Peng and Williams 1994). In addition, we present algorithms for applying these updates on-line during trials, unlike backward replay used by Lin (1993) that requires waiting until the end of each t...},
  file = {C\:\\Users\\Bingel\\Zotero\\storage\\NR3XBQJL\\Rummery and Niranjan - ON-LINE Q-LEARNING USING CONNECTIONIST SYSTEMS.pdf}
}

@book{russellArtificialIntelligenceModern2021,
  title = {Artificial Intelligence: A Modern Approach},
  shorttitle = {Artificial Intelligence},
  author = {Russell, Stuart J. and Norvig, Peter},
  date = {2021},
  series = {Pearson Series in Artificial Intelligence},
  edition = {Fourth edition},
  publisher = {{Pearson}},
  location = {{Hoboken}},
  abstract = {"Updated edition of popular textbook on Artificial Intelligence. This edition specific looks at ways of keeping artificial intelligence under control"--},
  isbn = {978-0-13-461099-3},
  keywords = {Artificial intelligence}
}

@inproceedings{samsudenReviewPaperImplementing2019,
  title = {A {{Review Paper}} on {{Implementing Reinforcement Learning Technique}} in {{Optimising Games Performance}}},
  booktitle = {2019 {{IEEE}} 9th {{International Conference}} on {{System Engineering}} and {{Technology}} ({{ICSET}})},
  author = {Samsuden, Mohd Azmin and Diah, Norizan Mat and Rahman, Nurazzah Abdul},
  date = {2019-10},
  pages = {258--263},
  publisher = {{IEEE}},
  location = {{Shah Alam, Malaysia}},
  doi = {10.1109/ICSEngT.2019.8906400},
  eventtitle = {2019 {{IEEE}} 9th {{International Conference}} on {{System Engineering}} and {{Technology}} ({{ICSET}})},
  isbn = {978-1-72810-758-5},
  file = {C\:\\Users\\Bingel\\Zotero\\storage\\DFIYEQTA\\Samsuden et al_2019_A Review Paper on Implementing Reinforcement Learning Technique in Optimising.pdf}
}

@article{suttonLearningPredictMethods1988,
  title = {Learning to {{Predict}} by the {{Methods}} of {{Temporal Differences}}},
  author = {Sutton, Richard S.},
  date = {1988},
  journaltitle = {Machine Learning},
  volume = {3},
  number = {1},
  pages = {9--44},
  issn = {08856125},
  doi = {10.1023/A:1022633531479},
  file = {C\:\\Users\\Bingel\\Zotero\\storage\\TGM8JV6J\\Learning to Predict by the Methods of Temporal Differences.pdf}
}

@article{suttonModernTheoryAdaptive1981,
  title = {Toward a Modern Theory of Adaptive Networks: Expectation and Prediction.},
  shorttitle = {Toward a Modern Theory of Adaptive Networks},
  author = {Sutton, R. and Barto, A.},
  date = {1981},
  journaltitle = {Psychological review},
  volume = {88},
  number = {2},
  pages = {135--170},
  doi = {10.1037/0033-295X.88.2.135},
  abstract = {The adaptive element presented learns to increase its response rate in anticipation of increased stimulation, producing a conditioned response before the occurrence of the unconditioned stimulus, and is in strong agreement with the behavioral data regarding the effects of stimulus context. Many adaptive neural network theories are based on neuronlike adaptive elements that can behave as single unit analogs of associative conditioning. In this article we develop a similar adaptive element, but one which is more closely in accord with the facts of animal learning theory than elements commonly studied in adaptive network research. We suggest that an essential feature of classical conditioning that has been largely overlooked by adaptive network theorists is its predictive nature. The adaptive element we present learns to increase its response rate in anticipation of increased stimulation, producing a conditioned response before the occurrence of the unconditioned stimulus. The element also is in strong agreement with the behavioral data regarding the effects of stimulus context, since it is a temporally refined extension of the Rescorla-Wagner model. We show by computer simulation that the element becomes sensitive to the most reliable, nonredundant, and earliest predictors of reinforcement . We also point out that the model solves many of the stability and saturation problems encountered in network simulations. Finally, we discuss our model in light of recent advances in the physiology and biochemistry of synaptic mechanisms.},
  file = {C\:\\Users\\Bingel\\Zotero\\storage\\K2RIV45J\\Sutton_Barto_1981_Toward a modern theory of adaptive networks.pdf}
}

@book{suttonReinforcementLearningIntroduction2018,
  title = {Reinforcement Learning: An Introduction},
  shorttitle = {Reinforcement Learning},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  date = {2018},
  series = {Adaptive Computation and Machine Learning Series},
  edition = {Second edition},
  publisher = {{The MIT Press}},
  location = {{Cambridge, Massachusetts}},
  abstract = {"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
  isbn = {978-0-262-03924-6},
  langid = {english},
  pagetotal = {526},
  keywords = {_tablet_modified,Reinforcement learning},
  file = {C\:\\Users\\Bingel\\Zotero\\storage\\PP56DKTH\\Sutton_Barto_2018_Reinforcement learning.pdf}
}

@book{szepesvariAlgorithmsReinforcementLearning2010,
  title = {Algorithms for Reinforcement Learning},
  author = {Szepesvari, Csaba},
  date = {2010},
  series = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
  number = {9},
  publisher = {{Morgan \& Claypool}},
  location = {{San Rafael, Calif.}},
  isbn = {978-1-60845-492-1},
  langid = {english},
  pagetotal = {89},
  file = {C\:\\Users\\Bingel\\Zotero\\storage\\JX2C6DM3\\Szepesvari_2010_Algorithms for reinforcement learning.pdf}
}

@incollection{szitaReinforcementLearningGames2012,
  title = {Reinforcement {{Learning}} in {{Games}}},
  booktitle = {Reinforcement {{Learning}}},
  author = {Szita, István},
  date = {2012},
  series = {Adaptation, {{Learning}}, and {{Optimization}}},
  volume = {12},
  pages = {539--577},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-27645-3_17},
  abstract = {Reinforcement learning and games have a long and mutually beneficial common history. From one side, games are rich and challenging domains for testing reinforcement learning algorithms. From the other side, in several games the best computer players use reinforcement learning. The chapter begins with a selection of games and notable reinforcement learning implementations. Without any modifications, the basic reinforcement learning algorithms are rarely sufficient for high-level gameplay, so it is essential to discuss the additional ideas, ways of inserting domain knowledge, implementation decisions that are necessary for scaling up. These are reviewed in sufficient detail to understand their potentials and their limitations. The second part of the chapter lists challenges for reinforcement learning in games, together with a review of proposed solution methods. While this listing has a game-centric viewpoint, and some of the items are specific to games (like opponent modelling), a large portion of this overview can provide insight for other kinds of applications, too. In the third part we review how reinforcement learning can be useful in game development and find its way into commercial computer games. Finally, we provide pointers for more in-depth reviews of specific games and solution approaches.},
  isbn = {978-3-642-27645-3},
  langid = {english},
  file = {C\:\\Users\\Bingel\\Zotero\\storage\\LJNUCBQS\\Szita - 2012 - Reinforcement Learning in Games.pdf}
}

@incollection{tesauroTemporalDifferenceLearning1992,
  title = {Temporal {{Difference Learning}} of {{Backgammon Strategy}}},
  booktitle = {Machine {{Learning Proceedings}} 1992},
  author = {Tesauro, Gerald},
  date = {1992},
  pages = {451--457},
  publisher = {{Elsevier}},
  doi = {10.1016/B978-1-55860-247-2.50063-2},
  isbn = {978-1-55860-247-2},
  langid = {english}
}

@article{watkinsLearningDelayedRewards1989a,
  title = {Learning {{From Delayed Rewards}}},
  author = {Watkins, Christopher J. C. H.},
  date = {1989-01-01},
  abstract = {Photocopy. Supplied by British Library. Thesis (Ph. D.)--King's College, Cambridge, 1989.},
  file = {C\:\\Users\\Bingel\\Zotero\\storage\\2A9BS78I\\Watkins_1989_Learning From Delayed Rewards.pdf}
}

@article{watkinsQlearning1992,
  title = {Q-Learning},
  author = {Watkins, Christopher J. C. H. and Dayan, Peter},
  date = {1992-05-01},
  journaltitle = {Machine Learning},
  shortjournal = {Mach Learn},
  volume = {8},
  number = {3},
  pages = {279--292},
  issn = {1573-0565},
  doi = {10.1007/BF00992698},
  abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states.},
  langid = {english},
  file = {C\:\\Users\\Bingel\\Zotero\\storage\\NL2F9TBG\\Watkins_Dayan_1992_Q-learning.pdf}
}


